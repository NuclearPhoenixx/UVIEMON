

\title{Test Specification}
\def \documentid {FLIGHTOS-UVIE-TS-001}
\date{Issue 1.0, May 1, 2016}

\newcommand\affil[1]{\textsuperscript#1}

\def\preparedby {Armin Luntzer\affil{1}}
\def\checkedby {Roland Ottensamer\affil{1}, Christian Reimers\affil{1}}
\def\approvedby {Franz Kerschbaum\affil{1}}

\def\affiliations{
	\affil{1} Department of Astrophysics, University of Vienna
}

\input{../shared/template/univie.tex}
\input{../shared/template/titlepage.tex}
\input{../shared/template/traceability.tex}
\input{../shared/glossary.tex}
\input{../shared/template/tests.tex}

\usepackage{enumitem}
\usepackage{vhistory}

\usepackage{biblatex}
\addbibresource{../shared/bibliography.bib}


\externaldocument[REQ-]{../testplan/testplan}
\input{../testplan/requirements.list}


\rereadauxdesignlabels


\begin{document}


\setmainfont{MyriadPro-SemiCondensed}
\uvietitlepage%
{Lean OS --\\ An operating system for the SSDP}%
{\doctitle}%
{../shared/images/logo2.pdf}
\setmainfont{MyriadPro}

\approvalpage

\tableofcontents
\newpage


\chapter*{List of Specified Tests}
\label{listoftests}
\the\designlist


\begin{versionhistory}
  \vhEntry{0.1}{01.05.2017}{AL}{Initial version}
  \vhEntry{1.0}{01.06.2017}{FK}{Document approved}
\end{versionhistory}


\chapter{Introduction}


\section{Purpose of the Document}

The test plan for the the FlightOS operating system is defined in
\cite{flightosTP}.\\
\\

\noindent
This test plan includes tests where the integrated operating
system is subjected to tests in its operational environment.\\

\noindent
This document specifies the acceptance tests for FlightOS.\\

\noindent
A test is specified by defining the:
\begin{itemize}
	\item pre-conditions for the test
	\item hardware baseline
	\item system configuration
	\item checks to be performed
	\item pass/fail criteria
\end{itemize}



\section{Test Overview}

A list of tests specified in this document is available in chapter:
\nameref{listoftests}


\chapter{Applicable and Reference Documents} % does not break automatically for some reason

\printbibliography[heading=none]


\chapter{Terms, Definitions and Abbreviated Items}

\printglossary[type=acronym]
\printglossary[type=main, style=altlist]


\chapter{Test Specification}

\section{Unit and Integration Tests}

\design{UIT-0001}
{Test Implementation}{%
For each software component or module, a corresponding unit or integration test
program is implemented by means of an adapted version of the \emph{kselftest}
framework. A shared collection of mockup and stub functions is maintained in
order to facilitate testing.
}%
{\meetsreq{UIT-0001, UITE-0001}}{}

\design{UIT-0002}
{Test Coverage}{%
All implemented unit tests deliver 100\% statement, decision and condition
coverage. Any deviations are justified as part of the unit test implementation.
}%
{\meetsreq{SCC-0004}}{}

\design{UIT-0003}
{Test Execution}{%
The collection of unit and integration tests is executable from the specified
source directory via the \emph{make} command.
}%
{\meetsreq{UIT-0003}}{}


\design{UIT-0004}
{Test Results}{%
The test executables generate human readable text output summarising the number,
types and outcome of each test performed.
}%
{\meetsreq{UIT-0004}}{}


\design{UIT-0005} % 0pt hspace fixes hyphenation
{\hspace{0pt} Automated  Testing}{%
The test executables report failed tests as shell exit codes, so the outcome
can be detected by an external test program such as \emph{git bisect}.
}%
{\meetsreq{UIT-0005}}{}

\design{UIT-0006}
{Test Suite}{%
All unit and integration tests, along with stub and mockup functions are stored
in the FlightOS source tree directory \mbox{\emph{tools/testing/unittest/}}.
}%
{\meetsreq{UIT-0002, UITE-0001}}{}



\section{Hardware-Software Tests}

\design{HST-0001}
{Test Implementation}{%
For each software component or module that needs verification in a hardware
environment, a test program is implemented.
}%
{\meetsreq{HST-0001}}{Since these tests typically involve more complex control %
via other tools (such as \emph{grmon} or \gls{SpaceWire} interfaces), they are %
implemented as shell scripts that set up the required configuration and %
executes the test on the target hardware. %
}


\design{HST-0002}
{Test Suite}{%
All hardware-software tests are stored in the FlightOS source
tree directory \mbox{\emph{tools/testing/hwtests/}}.%
}%
{\meetsreq{HST-0002}}{}


\design{HST-0003}
{Test Results}{%
The test executables generate human readable text output summarising the number,
types and outcome of each test performed.
}%
{\meetsreq{HST-0003}}{}


\design{HST-0004}
{\hspace{0pt} Automated Testing}{%
The test executables report failed tests as shell exit codes, so the outcome
can be detected by an external test program such as \emph{git bisect}.
}%
{\meetsreq{HST-0004}}{}


\design{SCC-0001}
{Code Coverage}{%
Statement, decision and condition coverage is determined via \emph{gcov}.
A \emph{make} target \emph{coverage\_test} is available in the \emph{Makefile}
of the unit test directory of the FlightOS source tree.
Human-readable reports are placed in the respective subdirectories for each
individual unit test.
}%
{\meetsreq{SCC-0001, SCC-0002, SCC-0003}}{}


\section{Acceptance Tests}

\design{ACT-0001}
{\hspace{0pt} Acceptance Test Definition}{%
Any acceptance test procedures are defined in chapter
\mbox{\nameref{detailedtestdefinitions}} of this document.
}%
{\meetsreq{ACT-0001}}{}

\design{ACT-0002}
{\hspace{0pt} Acceptance Test Use Cases}{%
The configuration and operational environment of FlightOS is defined for each
acceptance test of an identified use case.
}%
{\meetsreq{ACT-0002}}{}


\chapter{Detailed Test Definition}
\label{detailedtestdefinitions}

\section{Hardware-Software Test Procedures}
The following tables list the tests specified in this document. Note that in
the present version of the document, this list is \emph{incomplete}, as the
specification of the \gls{SSDP} has not yet been released at this time.\\

\noindent
Procedural steps are presented in tabular form and numerical sequence where
applicable. Column headings are \emph{TYPE}, \emph{SEQ}, \emph{Description}
and \emph{VERIFY}.\\

\noindent
Procedural step \emph{types} are:
\begin{description}[labelwidth=4em,leftmargin=\parindent,labelindent=\parindent]
	\item[\textbf{CMT}]	Comment
	\item[\textbf{PRE}]	Pre-condition
	\item[\textbf{PST}]	Post-condition
	\item[\textbf{STP}]	Step
\end{description}


\noindent
\emph{SEQ} indicates the sequence number of a step.\\

\noindent
\emph{Description} details the purpose and procedure of a step. \\

\noindent
\emph{VERIFY} lists any requirements or specifications verified in a 
step (if applicable).\\

\newpage
\section{Hardware-Software Test Procedures}

\subsection{Xentium Processing Network}

\begin{figure}%[htb]
\begin{center}
	\includegraphics[width=1.0\columnwidth]{../usermanual/images/task_network}
	\caption{Tasks in the processing network propagate according to the
		 operational code identifiers in their processing task list.
	 	 For details, see \cite{flightosUM}.}
	\label{fig:task_network}
\end{center}
\end{figure}

This is an integrated test of the \gls{Xentium} Processing Network
(see \fig{fig:task_network}). It loads a set of \gls{Xentium} program kernels in the
Xentium driver of FlightOS, which are then used to apply processing operations on
a series of input tasks to simulate a processing pipeline.\\

\noindent
The Xentium kernels used in this test are:

\begin{description}[labelwidth=4em,leftmargin=\parindent,labelindent=\parindent]
	\item[\textbf{deglitch}] applies a degliching operation
	\item[\textbf{dummy}]	 a simulation dummy that just passes on a task
	\item[\textbf{stack}]	 collects multiple tasks and stacks their data into a single task
	\item[\textbf{rampfit}]	 fits a linear function to a given number of samples
\end{description}

\noindent
The test generates a total of 12 tasks (data sets) as input in groups of 4 tasks
of 32, 64 and 64 samples of incremental value, containing a single glitch per
data set. The stacking parameter is set to 4 and the ramp length is set to 32
samples.\\

\noindent
The network then processes the tasks. Given 12 inputs, a stacking of 4 and a
ramp length of 32, the network outputs 1 data point output for the first 4 task
inputs of 32 samples and 2 data points for inputs of 4x64 samples each,
i.e. 5 data points in total. If this result is obtained, the test is considered
a success.\\




\testbegin{HWT-0001}{Integrated System Test of \gls{Xentium} Processing Network}{%
\testrow{CMT}{ }{This is a test to verify the function of the \gls{Xentium} %
		 Processing Network implementation by generating a series
	 	 of processing tasks and running them trough the processing
		 network.}{}%
\testrow{PRE}{ }{a \gls{MPPB} ver. 2.x hardware model}{}%
\testrow{PRE}{ }{\emph{grmon} ver. 1.x}{}%
\testrow{PRE}{ }{Ensure connection of \gls{MPPB} to test control host via serial %
		 cable on DEBUG UART to virtual device \emph{/dev/ttyS0} %
	 	 on the host platform.}{}%
\testrow{PRE}{ }{A checkout of the FlightOS source directory.}{}%
\testrow{PRE}{ }{\gls{MPPB}/\gls{Xentium} toolchain installed and configured}{}%
\testrow{STP}{1}{Ensure that the \gls{MPPB} is powered and reset.}{}%
\testrow{STP}{2}{Execute test script in the %
		 \mbox{\emph{tools/testing/hwtests/xen\_proc\_net}} %
		 subdirectory of the FlightOS source tree.}{}%
\testrow{STP}{3}{Verify that the program image is built.}{}%
\testrow{STP}{4}{Verify that the program image is uploaded to the \gls{MPPB} %
		 via \emph{grmon}}{}%
\testrow{STP}{5}{Verify that the program is started on the \gls{MPPB}.}{}%
\testrow{STP}{6}{Verify that the program reports SUCCESS on termination.}{}%
\testrow{CMT}{ }{Test completed.}
}



\chapter{Test Plan Requirements to Specification Traceability}

\noindent
Functional requirements are always reference to their specifications, others
only as needed or for clarification. This is reflected in the traceability
matrix.

\traceabilitymatrix

\end{document}
